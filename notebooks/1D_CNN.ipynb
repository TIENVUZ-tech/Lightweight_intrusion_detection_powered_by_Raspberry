{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cb0aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 23:27:05.916307: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-07 23:27:06.185962: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-07 23:27:07.524443: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Flatten, BatchNormalization, GlobalAveragePooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a893762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TensorFlow for CPU optimization\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "tf.config.set_soft_device_placement(True)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'dataset_path': \"../data/raw/CSE-CIC-IDS2018\",\n",
    "    'sample_size': 150000,  # Samples per file, set to None for all data\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'min_samples': 100,\n",
    "    'batch_size': 256,  # Larger batch for CNN\n",
    "    'epochs': 30,\n",
    "    'model_path': 'best_1dcnn_model.keras'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b401279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_sample_data(dataset_path, sample_size=None):\n",
    "    \"\"\"\n",
    "    Load CSV files with optional sampling.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to dataset directory\n",
    "        sample_size (int): Number of samples per file (None = all data)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataframe\n",
    "    \"\"\"\n",
    "    all_files = [\n",
    "        os.path.join(dataset_path, f) \n",
    "        for f in os.listdir(dataset_path) \n",
    "        if f.endswith('.csv')\n",
    "    ]\n",
    "    \n",
    "    if not all_files:\n",
    "        raise ValueError(f\"No CSV files found in {dataset_path}\")\n",
    "    \n",
    "    processed_frames = []\n",
    "    logger.info(f\"Starting to process {len(all_files)} files...\")\n",
    "    \n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file, low_memory=False)\n",
    "            \n",
    "            # Sample if specified\n",
    "            if sample_size and len(df) > sample_size:\n",
    "                df = df.sample(n=sample_size, random_state=CONFIG['random_state'])\n",
    "            \n",
    "            processed_frames.append(df)\n",
    "            logger.info(f\"Processed: {os.path.basename(file)} ({len(df)} rows)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not processed_frames:\n",
    "        raise ValueError(\"No files were successfully processed\")\n",
    "    \n",
    "    logger.info(\"Concatenating dataframes...\")\n",
    "    combined_df = pd.concat(processed_frames, ignore_index=True)\n",
    "    logger.info(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b319f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Clean and prepare data for modeling.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Raw dataframe\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y, label_counts, label_encoder)\n",
    "    \"\"\"\n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = [\n",
    "        'Flow ID', 'Source IP', 'Source Port', \n",
    "        'Destination IP', 'Destination Port', 'Timestamp'\n",
    "    ]\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())\n",
    "    \n",
    "    # Replace infinite values\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    label_encoder = LabelEncoder()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col == 'Label':\n",
    "            df[col] = label_encoder.fit_transform(df[col])\n",
    "        else:\n",
    "            temp_encoder = LabelEncoder()\n",
    "            df[col] = temp_encoder.fit_transform(df[col])\n",
    "    \n",
    "    # Check label distribution\n",
    "    logger.info(\"Original Label Distribution:\")\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    logger.info(f\"\\n{label_counts}\")\n",
    "    \n",
    "    # Separate features and labels\n",
    "    X = df.drop('Label', axis=1).values\n",
    "    y = df['Label'].values\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y, label_counts, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024a438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_cnn(X, y, label_counts, min_samples=100):\n",
    "    \"\"\"\n",
    "    Prepare data for 1D-CNN model training.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Feature matrix\n",
    "        y (np.array): Labels\n",
    "        label_counts (pd.Series): Label distribution\n",
    "        min_samples (int): Minimum samples per class\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test, num_classes)\n",
    "    \"\"\"\n",
    "    # Filter classes with sufficient samples\n",
    "    valid_classes = label_counts[label_counts >= min_samples].index.tolist()\n",
    "    mask = np.isin(y, valid_classes)\n",
    "    X_filtered = X[mask]\n",
    "    y_filtered = y[mask]\n",
    "    \n",
    "    logger.info(f\"\\nFiltered to {len(valid_classes)} classes\")\n",
    "    unique, counts = np.unique(y_filtered, return_counts=True)\n",
    "    logger.info(f\"Class distribution: {dict(zip(unique, counts))}\")\n",
    "    \n",
    "    # Balance data with SMOTE\n",
    "    logger.info(\"Applying SMOTE for class balancing...\")\n",
    "    smote = SMOTE(random_state=CONFIG['random_state'])\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_filtered, y_filtered)\n",
    "    \n",
    "    # Convert labels to categorical\n",
    "    y_resampled_cat = to_categorical(y_resampled)\n",
    "    num_classes = y_resampled_cat.shape[1]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_resampled, y_resampled_cat, \n",
    "        test_size=CONFIG['test_size'], \n",
    "        random_state=CONFIG['random_state'],\n",
    "        stratify=y_resampled\n",
    "    )\n",
    "    \n",
    "    # Reshape for 1D-CNN: (samples, timesteps, features)\n",
    "    # For network traffic, we treat each feature as a time step\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    \n",
    "    logger.info(f\"Training set shape: {X_train.shape}\")\n",
    "    logger.info(f\"Test set shape: {X_test.shape}\")\n",
    "    logger.info(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d483b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_1dcnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create optimized 1D-CNN model for network intrusion detection.\n",
    "    \n",
    "    1D-CNN is ideal for:\n",
    "    - Sequential pattern detection in network traffic\n",
    "    - Fast inference (5-10x faster than LSTM/GRU)\n",
    "    - Better feature extraction from flow-based data\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input data\n",
    "        num_classes (int): Number of output classes\n",
    "    \n",
    "    Returns:\n",
    "        Sequential: Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First Conv Block - Extract low-level patterns\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', \n",
    "               padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Second Conv Block - Extract mid-level patterns\n",
    "        Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Third Conv Block - Extract high-level patterns\n",
    "        Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Global pooling instead of Flatten (reduces parameters)\n",
    "        GlobalAveragePooling1D(),\n",
    "        \n",
    "        # Dense layers for classification\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile with Adam optimizer\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"1D-CNN MODEL ARCHITECTURE\")\n",
    "    logger.info(\"=\"*60)\n",
    "    model.summary(print_fn=logger.info)\n",
    "    logger.info(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e458677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train the 1D-CNN model with optimized callbacks.\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model\n",
    "        X_train, X_test: Training and test features\n",
    "        y_train, y_test: Training and test labels\n",
    "    \n",
    "    Returns:\n",
    "        History: Training history\n",
    "    \"\"\"\n",
    "    # Setup callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        CONFIG['model_path'],\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"STARTING TRAINING\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=CONFIG['epochs'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd63456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, label_encoder):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with metrics and confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_test, y_test: Test data\n",
    "        label_encoder: Label encoder for class names\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"MODEL EVALUATION\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    logger.info(f\"Test Loss: {results[0]:.4f}\")\n",
    "    logger.info(f\"Test Accuracy: {results[1]*100:.2f}%\")\n",
    "    logger.info(f\"Test Precision: {results[2]*100:.2f}%\")\n",
    "    logger.info(f\"Test Recall: {results[3]*100:.2f}%\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Classification Report\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"CLASSIFICATION REPORT\")\n",
    "    logger.info(\"=\"*60)\n",
    "    try:\n",
    "        class_names = label_encoder.inverse_transform(np.unique(y_test_classes))\n",
    "        report = classification_report(y_test_classes, y_pred_classes, \n",
    "                                       target_names=class_names)\n",
    "        logger.info(f\"\\n{report}\")\n",
    "    except:\n",
    "        report = classification_report(y_test_classes, y_pred_classes)\n",
    "        logger.info(f\"\\n{report}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "    plt.title('Confusion Matrix - 1D-CNN Model', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix_1dcnn.png', dpi=300, bbox_inches='tight')\n",
    "    logger.info(\"Confusion matrix saved as 'confusion_matrix_1dcnn.png'\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c030d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Create comprehensive training history plots.\n",
    "    \n",
    "    Args:\n",
    "        history: Training history object\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0, 0].legend(loc='lower right')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0, 1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0, 1].legend(loc='upper right')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision plot\n",
    "    axes[1, 0].plot(history.history['precision'], label='Training Precision', linewidth=2)\n",
    "    axes[1, 0].plot(history.history['val_precision'], label='Validation Precision', linewidth=2)\n",
    "    axes[1, 0].set_title('Model Precision', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Precision', fontsize=12)\n",
    "    axes[1, 0].legend(loc='lower right')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Recall plot\n",
    "    axes[1, 1].plot(history.history['recall'], label='Training Recall', linewidth=2)\n",
    "    axes[1, 1].plot(history.history['val_recall'], label='Validation Recall', linewidth=2)\n",
    "    axes[1, 1].set_title('Model Recall', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Recall', fontsize=12)\n",
    "    axes[1, 1].legend(loc='lower right')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history_1dcnn.png', dpi=300, bbox_inches='tight')\n",
    "    logger.info(\"Training plots saved as 'training_history_1dcnn.png'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af33f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"1D-CNN NETWORK INTRUSION DETECTION SYSTEM\")\n",
    "        logger.info(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        # Step 1: Load data\n",
    "        combined_df = load_and_sample_data(\n",
    "            CONFIG['dataset_path'],\n",
    "            CONFIG['sample_size']\n",
    "        )\n",
    "        \n",
    "        # Step 2: Preprocess\n",
    "        X, y, label_counts, label_encoder = preprocess_data(combined_df)\n",
    "        \n",
    "        # Step 3: Prepare for CNN\n",
    "        X_train, X_test, y_train, y_test, num_classes = prepare_data_for_cnn(\n",
    "            X, y, label_counts,\n",
    "            min_samples=CONFIG['min_samples']\n",
    "        )\n",
    "        \n",
    "        # Step 4: Create 1D-CNN model\n",
    "        model = create_1dcnn_model(\n",
    "            (X_train.shape[1], X_train.shape[2]),\n",
    "            num_classes\n",
    "        )\n",
    "        \n",
    "        # Step 5: Train\n",
    "        history = train_model(model, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Step 6: Evaluate\n",
    "        evaluate_model(model, X_test, y_test, label_encoder)\n",
    "        \n",
    "        # Step 7: Plot training history\n",
    "        plot_training_history(history)\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"✅ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"Model saved at: {CONFIG['model_path']}\")\n",
    "        logger.info(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cfd41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 23:27:08,294 - INFO - \n",
      "============================================================\n",
      "2025-11-07 23:27:08,295 - INFO - 1D-CNN NETWORK INTRUSION DETECTION SYSTEM\n",
      "2025-11-07 23:27:08,295 - INFO - ============================================================\n",
      "\n",
      "2025-11-07 23:27:08,296 - INFO - Starting to process 14 files...\n",
      "2025-11-07 23:27:08,557 - INFO - Processed: DoS attacks-Slowloris.csv (36754 rows)\n",
      "2025-11-07 23:27:08,568 - INFO - Processed: Brute Force -Web.csv (2073 rows)\n",
      "2025-11-07 23:27:24,379 - INFO - Processed: DDoS attacks-LOIC-HTTP.csv (150000 rows)\n",
      "2025-11-07 23:27:24,403 - INFO - Processed: DDOS attack-LOIC-UDP.csv (5784 rows)\n",
      "2025-11-07 23:27:27,620 - INFO - Processed: DoS attacks-SlowHTTPTest.csv (150000 rows)\n",
      "2025-11-07 23:27:31,575 - INFO - Processed: Infilteration.csv (150000 rows)\n",
      "2025-11-07 23:27:31,587 - INFO - Processed: Brute Force -XSS.csv (734 rows)\n",
      "2025-11-07 23:27:32,614 - INFO - Processed: DoS attacks-GoldenEye.csv (139922 rows)\n",
      "2025-11-07 23:27:37,057 - INFO - Processed: FTP-BruteForce.csv (150000 rows)\n",
      "2025-11-07 23:27:41,609 - INFO - Processed: SSH-Bruteforce.csv (150000 rows)\n",
      "2025-11-07 23:27:41,612 - INFO - Processed: SQL Injection.csv (286 rows)\n",
      "2025-11-07 23:27:48,744 - INFO - Processed: Bot.csv (150000 rows)\n",
      "2025-11-07 23:27:58,894 - INFO - Processed: DoS attacks-Hulk.csv (150000 rows)\n",
      "2025-11-07 23:28:14,303 - INFO - Processed: DDOS attack-HOIC.csv (150000 rows)\n",
      "2025-11-07 23:28:14,304 - INFO - Concatenating dataframes...\n",
      "2025-11-07 23:28:14,427 - INFO - Combined dataset shape: (1385553, 79)\n",
      "2025-11-07 23:28:18,627 - INFO - Original Label Distribution:\n",
      "2025-11-07 23:28:18,641 - INFO - \n",
      "Label\n",
      "0     983015\n",
      "14     44959\n",
      "9      44783\n",
      "12     44348\n",
      "8      44347\n",
      "4      44046\n",
      "11     43956\n",
      "1      42215\n",
      "7      41508\n",
      "6      38728\n",
      "10     10990\n",
      "5       1730\n",
      "2        611\n",
      "3        230\n",
      "13        87\n",
      "Name: count, dtype: int64\n",
      "2025-11-07 23:28:19,659 - INFO - \n",
      "Filtered to 14 classes\n",
      "2025-11-07 23:28:19,677 - INFO - Class distribution: {0: 983015, 1: 42215, 2: 611, 3: 230, 4: 44046, 5: 1730, 6: 38728, 7: 41508, 8: 44347, 9: 44783, 10: 10990, 11: 43956, 12: 44348, 14: 44959}\n",
      "2025-11-07 23:28:19,678 - INFO - Applying SMOTE for class balancing...\n",
      "/home/icnlab/anaconda3/envs/detectron/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "2025-11-07 23:28:45,566 - INFO - Training set shape: (11009768, 78, 1)\n",
      "2025-11-07 23:28:45,567 - INFO - Test set shape: (2752442, 78, 1)\n",
      "2025-11-07 23:28:45,567 - INFO - Number of classes: 15\n",
      "/home/icnlab/anaconda3/envs/detectron/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762532925.687991 2504507 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1762532925.728309 2504507 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-11-07 23:28:45,823 - INFO - \n",
      "============================================================\n",
      "2025-11-07 23:28:45,823 - INFO - 1D-CNN MODEL ARCHITECTURE\n",
      "2025-11-07 23:28:45,823 - INFO - ============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 23:28:45,838 - INFO - Model: \"sequential\"\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv1d (Conv1D)                 │ (None, 78, 128)        │           512 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (None, 78, 128)        │           512 │\n",
      "│ (BatchNormalization)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ conv1d_1 (Conv1D)               │ (None, 78, 128)        │        49,280 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_1           │ (None, 78, 128)        │           512 │\n",
      "│ (BatchNormalization)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling1d (MaxPooling1D)    │ (None, 39, 128)        │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout (Dropout)               │ (None, 39, 128)        │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ conv1d_2 (Conv1D)               │ (None, 39, 256)        │        98,560 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_2           │ (None, 39, 256)        │         1,024 │\n",
      "│ (BatchNormalization)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ conv1d_3 (Conv1D)               │ (None, 39, 256)        │       196,864 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_3           │ (None, 39, 256)        │         1,024 │\n",
      "│ (BatchNormalization)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling1d_1 (MaxPooling1D)  │ (None, 19, 256)        │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout_1 (Dropout)             │ (None, 19, 256)        │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ conv1d_4 (Conv1D)               │ (None, 19, 512)        │       393,728 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_4           │ (None, 19, 512)        │         2,048 │\n",
      "│ (BatchNormalization)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling1d_2 (MaxPooling1D)  │ (None, 9, 512)         │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout_2 (Dropout)             │ (None, 9, 512)         │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling1d        │ (None, 512)            │             0 │\n",
      "│ (GlobalAveragePooling1D)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (Dense)                   │ (None, 256)            │       131,328 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_5           │ (None, 256)            │         1,024 │\n",
      "│ (BatchNormalization)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout_3 (Dropout)             │ (None, 256)            │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense_1 (Dense)                 │ (None, 128)            │        32,896 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_6           │ (None, 128)            │           512 │\n",
      "│ (BatchNormalization)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout_4 (Dropout)             │ (None, 128)            │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense_2 (Dense)                 │ (None, 15)             │         1,935 │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      " Total params: 911,759 (3.48 MB)\n",
      " Trainable params: 908,431 (3.47 MB)\n",
      " Non-trainable params: 3,328 (13.00 KB)\n",
      "\n",
      "2025-11-07 23:28:45,839 - INFO - ============================================================\n",
      "\n",
      "2025-11-07 23:28:45,839 - INFO - \n",
      "============================================================\n",
      "2025-11-07 23:28:45,839 - INFO - STARTING TRAINING\n",
      "2025-11-07 23:28:45,840 - INFO - ============================================================\n",
      "2025-11-07 23:28:47.076724: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 3435047616 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8606 - loss: 0.2901 - precision: 0.8713 - recall: 0.8522\n",
      "Epoch 1: val_accuracy improved from -inf to 0.86662, saving model to best_1dcnn_model.keras\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8632s\u001b[0m 201ms/step - accuracy: 0.8606 - loss: 0.2901 - precision: 0.8713 - recall: 0.8522 - val_accuracy: 0.8666 - val_loss: 0.3255 - val_precision: 0.8668 - val_recall: 0.8665 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8963 - loss: 0.1884 - precision: 0.8970 - recall: 0.8955\n",
      "Epoch 2: val_accuracy improved from 0.86662 to 0.90015, saving model to best_1dcnn_model.keras\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8548s\u001b[0m 199ms/step - accuracy: 0.8963 - loss: 0.1884 - precision: 0.8970 - recall: 0.8955 - val_accuracy: 0.9001 - val_loss: 0.1778 - val_precision: 0.9004 - val_recall: 0.8999 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8912 - loss: 0.1993 - precision: 0.8917 - recall: 0.8906\n",
      "Epoch 3: val_accuracy did not improve from 0.90015\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8552s\u001b[0m 199ms/step - accuracy: 0.8912 - loss: 0.1993 - precision: 0.8917 - recall: 0.8906 - val_accuracy: 0.8347 - val_loss: 0.5078 - val_precision: 0.8356 - val_recall: 0.8339 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8998 - loss: 0.1818 - precision: 0.9001 - recall: 0.8993\n",
      "Epoch 4: val_accuracy did not improve from 0.90015\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8519s\u001b[0m 198ms/step - accuracy: 0.8998 - loss: 0.1818 - precision: 0.9001 - recall: 0.8993 - val_accuracy: 0.8480 - val_loss: 0.4600 - val_precision: 0.8481 - val_recall: 0.8478 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.9012 - loss: 0.1786 - precision: 0.9015 - recall: 0.9008\n",
      "Epoch 5: val_accuracy improved from 0.90015 to 0.90390, saving model to best_1dcnn_model.keras\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8529s\u001b[0m 198ms/step - accuracy: 0.9012 - loss: 0.1786 - precision: 0.9015 - recall: 0.9008 - val_accuracy: 0.9039 - val_loss: 0.1740 - val_precision: 0.9039 - val_recall: 0.9039 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.9038 - loss: 0.1741 - precision: 0.9041 - recall: 0.9035\n",
      "Epoch 6: val_accuracy did not improve from 0.90390\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8888s\u001b[0m 207ms/step - accuracy: 0.9038 - loss: 0.1741 - precision: 0.9041 - recall: 0.9035 - val_accuracy: 0.8120 - val_loss: 0.8675 - val_precision: 0.8199 - val_recall: 0.8034 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m43007/43007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9073 - loss: 0.1682 - precision: 0.9076 - recall: 0.9070"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
